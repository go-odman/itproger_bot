import requests
from bs4 import BeautifulSoup
from typing import List, Dict
import re
from urllib.parse import urljoin
from dotenv import load_dotenv
import os

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()

class ITProgerParser:
    def __init__(self):
        self.base_url = "https://itproger.com"
        self.news_url = os.getenv('ITPROGER_URL', 'https://itproger.com/news')

    def get_news_list(self) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(self.news_url, headers=headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            news_items = []
            
            # –ò—â–µ–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ
            articles = soup.select('.allArticles .article')
            
            for article in articles:
                try:
                    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫–∞
                    title_link = article.find('a')
                    if not title_link:
                        continue
                    
                    title = title_link.get_text().strip()
                    link = urljoin(self.base_url, title_link['href'])
                    
                    # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    img_elem = title_link.find('img')
                    image_url = ""
                    if img_elem and img_elem.get('src'):
                        image_url = urljoin(self.base_url, img_elem['src'])
                    
                    # –û–ø–∏—Å–∞–Ω–∏–µ
                    desc_elem = article.find('span')
                    description = ""
                    if desc_elem:
                        # –ë–µ—Ä–µ–º —Å–ª–µ–¥—É—é—â–∏–π span –ø–æ—Å–ª–µ —Å—Å—ã–ª–∫–∏ —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
                        spans = article.find_all('span')
                        if len(spans) > 1:
                            description = spans[1].get_text().strip()
                    
                    # –î–∞—Ç–∞ –∏ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã
                    meta_div = article.find('div')
                    date = ""
                    views = ""
                    if meta_div:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É
                        time_elem = meta_div.find('span', class_='time')
                        if time_elem:
                            date = time_elem.get_text().strip()
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–æ—Å–º–æ—Ç—Ä—ã
                        eye_elem = meta_div.find('i', class_='fa-eye')
                        if eye_elem and eye_elem.parent:
                            views_text = eye_elem.parent.get_text().strip()
                            views = re.search(r'(\d+)', views_text)
                            views = views.group(1) if views else "0"
                    
                    if title and link:
                        news_items.append({
                            'title': title,
                            'url': link,
                            'image_url': image_url,
                            'description': description,
                            'date': date,
                            'views': views
                        })
                        
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç–∞—Ç—å–∏: {e}")
                    continue
                    
            return news_items
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []

    def get_article_content(self, article_url: str) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(article_url, headers=headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = soup.find('h1')
            title = title_elem.get_text().strip() if title_elem else ""
            
            # –û—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ - –∏—â–µ–º –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            main_content = soup.find('main') or soup.find('div', class_=re.compile(r'content|article'))
            
            content = ""
            if main_content:
                # –£–±–∏—Ä–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (—Ä–µ–∫–ª–∞–º–∞, –±–æ–∫–æ–≤—ã–µ –ø–∞–Ω–µ–ª–∏ –∏ —Ç.–¥.)
                for unwanted in main_content.find_all(['script', 'style', 'aside', 'nav']):
                    unwanted.decompose()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –±–ª–æ–∫–∏
                text_blocks = main_content.find_all(['p', 'h2', 'h3', 'h4', 'pre', 'code', 'ul', 'ol'])
                
                for block in text_blocks:
                    if block.name in ['h2', 'h3', 'h4']:
                        text = block.get_text().strip()
                        if text:
                            content += f"**{text}**\n\n"
                    elif block.name == 'pre':
                        code_text = block.get_text().strip()
                        content += f"```\n{code_text}\n```\n\n"
                    elif block.name == 'code':
                        code_text = block.get_text().strip()
                        content += f"`{code_text}` "
                    elif block.name in ['ul', 'ol']:
                        items = block.find_all('li')
                        for item in items:
                            text = item.get_text().strip()
                            if text:
                                content += f"‚Ä¢ {text}\n"
                        content += "\n"
                    else:  # p –∏ –¥—Ä—É–≥–∏–µ
                        text = block.get_text().strip()
                        if text and len(text) > 10:  # –§–∏–ª—å—Ç—Ä—É–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
                            content += f"{text}\n\n"
            
            # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
            if not content.strip():
                content = "üìù –ü–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏ –¥–æ—Å—Ç—É–ø–Ω–æ –Ω–∞ —Å–∞–π—Ç–µ itProger.\n\n" \
                         f"[–ß–∏—Ç–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é]({article_url})"
            
            return {
                'title': title,
                'full_content': content.strip(),
                'url': article_url
            }
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç–∞—Ç—å–∏: {e}")
            return {
                'title': "", 
                'full_content': f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏\n\n[–ß–∏—Ç–∞—Ç—å –Ω–∞ —Å–∞–π—Ç–µ]({article_url})", 
                'url': article_url
            }

    def search_articles(self, query: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        all_articles = self.get_news_list()
        if not all_articles:
            return []
        
        query_lower = query.lower()
        filtered = [
            article for article in all_articles
            if query_lower in article['title'].lower() or 
               query_lower in article.get('description', '').lower()
        ]
        
        return filtered

    def _extract_code_blocks(self, text: str) -> str:
        """–ò–∑–≤–ª–µ—á—å –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –±–ª–æ–∫–∏ –∫–æ–¥–∞ –¥–ª—è markdown"""
        return text